import jax
import jax.numpy as np
from jax import jit, grad, vmap
from typing import Sequence
from flax import linen
import pickle
import os
import imageio

from data_preprocessing import load_data


model_name = "test"
SAVE = True

key = jax.random.PRNGKey(42)

# v0.1
input_shape = (96, 96, 2)
outputs_features = (16, 32, 64, 256)
kernels_size = ((3, 3), (5, 5), (5, 5))
strides = ((1, 1), (2, 2), (2, 2))
padding = "VALID"


""" Functions for loss computation """


@jit
def expected_Jaccard_index(px1, px2, eps=1e-8):
    """Expected Jaccard index between two samples taken from the input from the input multivariate Bernoulli distirbutions

    Args:
        px1 (ndarray): probability of activation of the first multivariate Bernoulli
        px2 (ndarray): probability of activation of the second multivariate Bernoulli
        eps (float, optional): small epsilon added to each probability to avoid division by zero. Defaults to 1e-8.

    Returns:
        (ndarray): expected Jaccard index
    """
    return (((px1 + eps) * (px2 + eps))).sum(axis=-1) / (
        1 - (1 - (px1 + eps)) * (1 - (px2 + eps))
    ).sum(axis=-1)


def gen_flo_loss(model):
    @jit
    def flo_loss(params, xs):
        # Note: all patches are treated independently
        # TODO: check what happens by flatting patches and batches all together, maybe it works nicely.
        # The only issue could be cause by the fact that different patches could have the same input.
        # In such case, the loss would try to separate the two representations, but they should actually be the same.
        pys = model.apply(params, xs)

        # # flattened shape =  (batch*width*height, output_features)
        # pys = pys.reshape(-1, pys.shape[-1])

        # For each patch, separately, compute the expected number of shared bits (= shared active output features)
        # between all the outputs generated on this single patch
        # p(x'|x) = Expected Jaccard Index between the output distributions generated by the two
        ps = expected_Jaccard_index(pys[:, None], pys[None,], eps=0.5 / (pys.shape[-1]))

        gs = np.log(ps + 1e-6)
        gs_self = np.diagonal(gs.T, axis1=-1, axis2=-2).T

        e_p = (1.0 / (xs.shape[0] - 1)) * ((np.exp(gs - gs_self)).sum(axis=1) - 1.0)

        us = -np.log(1.0 / (e_p + 1e-6))

        # # FLO
        flo = 1 - (us + np.exp(-us) * e_p)

        return -flo.mean()

    return flo_loss


""" Import data """


train_df, test_df = load_data()

print(train_df.columns)
print(train_df.index)
# X = train_df.loc[""]

# """ Define the model """


# class CRBM(linen.Module):
#     """A simple CNN model with one final dense layer"""

#     outputs_features: Sequence[int]
#     kernel_size: Sequence[Sequence[int]]
#     strides: Sequence[Sequence[int]]
#     padding: str

#     def setup(self):
#         self.num_layers = len(self.outputs_features)
#         self.conv_layers = [
#             linen.Conv(  # Convolutional layer
#                 self.outputs_features[i],
#                 self.kernel_size[i],
#                 self.strides[i],
#                 self.padding,
#             )
#             for i in range(self.num_layers - 1)
#         ]
#         self.dense_layer = linen.Dense(self.outputs_features[-1])  # Dense layer

#     def __call__(self, x):
#         for i in range(self.num_layers - 1):
#             x = self.conv_layers[i](x)
#             x = linen.relu(x)
#             x = linen.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
#         x = x.reshape((x.shape[0], -1))
#         x = self.dense_layer(x)
#         x = linen.sigmoid(x)
#         return x


# if __name__ == "__main__":
#     ### Import datasets

#     base_path = os.path.join(
#         os.path.dirname(__file__),
#         os.path.pardir,
#         os.path.pardir,
#         "resources",
#         "datasets",
#         "car-racing",
#         "2023-05-22",
#     )

#     X, A, R = load_episodes(base_path)
#     # Reshape to (batch, width, height, channels)
#     X = X.astype(np.float32).reshape(-1, 96, 96, 1)
#     # rescale in [0, 1]
#     X /= 255.0

#     ### Compute an event-like input from the images, and attach it to the actual images
#     DX = np.diff(X, axis=0, prepend=0.0)
#     # Binarize
#     DX_bin = (np.abs(DX) > 0.001).astype(float)
#     # Append
#     X_comp = np.concatenate([X, DX_bin], axis=-1)

#     """ Plot two images and their difference """

#     # import plotly.graph_objects as go
#     # from plotly.subplots import make_subplots

#     # n_rows = 4
#     # n_cols = 3
#     # fig = make_subplots(rows=n_rows, cols=n_cols)
#     # idx_tmp = 3000
#     # skip_frames = 10
#     # for i in range(n_rows):
#     #     fig.add_trace(
#     #         go.Heatmap(
#     #             z=np.flip(X_comp[i * skip_frames + idx_tmp, ..., 0], axis=0),
#     #             colorscale="Greys",
#     #             zmin=0,
#     #             zmax=1,
#     #         ),
#     #         row=i + 1,
#     #         col=1,
#     #     )
#     #     fig.add_trace(
#     #         go.Heatmap(
#     #             z=np.flip(X_comp[1 + i * skip_frames + idx_tmp, ..., 0], axis=0),
#     #             colorscale="Greys",
#     #             zmin=0,
#     #             zmax=1,
#     #         ),
#     #         row=i + 1,
#     #         col=2,
#     #     )

#     #     fig.add_trace(
#     #         go.Heatmap(
#     #             z=np.flip(X_comp[i * skip_frames + idx_tmp, ..., 1], axis=0),
#     #             colorscale="Greys",
#     #             zmin=0,
#     #             zmax=1,
#     #         ),
#     #         row=i + 1,
#     #         col=3,
#     #     )

#     # fig.update_layout(
#     #     height=n_rows * 150,
#     #     width=n_cols * 150,
#     #     title_text="Sample data. One row = (X(t), X(t+1), DX(t+1))",
#     # )
#     # fig.show()

#     ### Init encoder

#     encoder = CRBM(outputs_features, kernels_size, strides, padding)

#     """ Try to import parameters from model which was already trained, otherwise initialize new parameters. """

#     params_base_path = os.path.join(
#         os.path.dirname(__file__),
#         os.path.pardir,
#         os.path.pardir,
#         "resources",
#         "trained_models",
#         "car-racing",
#     )

#     # Check if parameters are already available
#     if os.path.exists(os.path.join(params_base_path, model_name + ".pkl")):
#         with open(os.path.join(params_base_path, model_name + ".pkl"), "rb") as f:
#             encoder_params = pickle.load(f)
#     else:
#         # Initialize parameters
#         key, _ = jax.random.split(key)
#         x = np.ones((1, 96, 96, 2))
#         encoder_params = encoder.init(key, x)

#     # Apply model
#     # this also checks if the model is correctly initialized and the params are of the correct dimension
#     z = encoder.apply(encoder_params, X_comp[10:20])

#     """ Train the model."""

#     ## Init Optimizer
#     import optax

#     epochs = 500
#     n_batch = 128

#     history = {
#         "loss_train": [
#             [],
#         ],
#         "loss_test": [],
#     }

#     print_batches = 25

#     # Init Optimizer
#     learning_rate = 1e-3
#     optimizer = optax.adam(learning_rate)
#     opt_state = optimizer.init(encoder_params)

#     # Loss function
#     f_loss = gen_flo_loss(encoder)

#     try:
#         for e in range(epochs):
#             # append empty list for the loss during this epoch
#             history["loss_train"][-1].append([])
#             mean_loss_last_batches = 0.0

#             # shuffle datasets
#             key, _ = jax.random.split(key)
#             X_comp = jax.random.permutation(key, X_comp, axis=0, independent=False)

#             # train in minibatches
#             for batch in range(int(X_comp.shape[0] / n_batch)):
#                 if (batch % print_batches == 0) and (batch != 0):
#                     print(
#                         f"Epoch: {e:03d}. Batch {batch}/{int(X_comp.shape[0]/n_batch)} Mean loss on last {print_batches} batches: {mean_loss_last_batches}"
#                     )
#                     mean_loss_last_batches = 0.0

#                 # select inputs for this batch
#                 xs = X_comp[batch * n_batch : (batch + 1) * n_batch]
#                 # ys = Y[batch*n_batch : (batch+1)*n_batch]

#                 # # Compute weights to use for loss
#                 # ws = (np.logical_not(ys[:, None] == ys[None, :])).astype(np.float32)

#                 # apply noise
#                 key, subkey, noise_key = jax.random.split(key, num=3)
#                 noise_level = jax.random.uniform(noise_key, minval=0.0, maxval=0.1)
#                 noise_positive = (jax.random.uniform(key, xs.shape) < 0.1).astype(
#                     np.float32
#                 )
#                 noise_negative = (jax.random.uniform(subkey, xs.shape) < 0.1).astype(
#                     np.float32
#                 )
#                 xs = np.clip(xs + 0.5 * noise_positive - 0.5 * noise_negative, 0.0, 1.0)

#                 # Compute loss and gradient

#                 loss, grads = jax.value_and_grad(f_loss, 0)(encoder_params, xs)
#                 updates, opt_state = optimizer.update(grads, opt_state)
#                 encoder_params = optax.apply_updates(encoder_params, updates)

#                 history["loss_train"][-1].append(loss)

#                 mean_loss_last_batches = mean_loss_last_batches + loss / print_batches

#     except KeyboardInterrupt:
#         print("Training interrupted, saving model...")

#     """ Save model """

#     if SAVE:
#         # Save model, to be used for testing of Temporal Learning
#         filepath = os.path.join(params_base_path, model_name + ".pkl")
#         with open(filepath, "wb") as f:
#             pickle.dump(encoder_params, f)
